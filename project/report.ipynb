{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relation Between Climate Change and Immigration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Question: Does climate change have an affect on immigration in countries that are vulnerable to climate change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   In this project I want to examine two of the biggest problems that world is facing now and probably will be facing in the future. Without doubt \"climate change\" is a buzzword that is used in every field. We hear this term in economy, sociology, politics, agriculture besides others. One other problem which \n",
    "has arisen in the last couple of years is \"Immigration\". According to IOM-UN (International Organization for Immigration), in 2020 there were 281 million international immigrants which is 3.6 percent of the global population. Today, much bigger numbers are estimated.\n",
    "\n",
    "   There are many different reasons for immigration. UN states on its own website \"Others move to escape conflict, persecution or large-scale human rights violations. Still others move in response to the adverse effects of climate change, natural disasters or other environmental factors.\n",
    "Today, more people than ever live in a country other than the one in which they were born.\" In this project I aim to study the affects of climate change on immigration. I will focus on the countries which are more vulnerable to climate change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project I have chosen 2 datasets. Both of them are available in Kaggle.\n",
    "First dataset is about global temperature. In this dataset 5 csv files are available which has different classification. For my project I chose the Global Land Temperature By Country. (Link : https://www.kaggle.com/datasets/berkeleyearth/climate-change-earth-surface-temperature-data?select=GlobalLandTemperaturesByCountry.csv)\n",
    "\n",
    "This dataset has 4 columns. \"dt\", \"AverageTemperature\", \"AverageTemperatureUncertainty\" and \"Country\". Between 1743 and 2013 all monthly values are stored in this dataset. General informations about the dataset is shown below.\n",
    "This dataset has a licence of CC BY-NC-SA 4.0 which enables me to use in my project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>4.384</td>\n",
       "      <td>2.294</td>\n",
       "      <td>Åland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Åland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Åland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Åland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Åland</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty Country\n",
       "0  1743-11-01               4.384                          2.294   Åland\n",
       "1  1743-12-01                 NaN                            NaN   Åland\n",
       "2  1744-01-01                 NaN                            NaN   Åland\n",
       "3  1744-02-01                 NaN                            NaN   Åland\n",
       "4  1744-03-01                 NaN                            NaN   Åland"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temperature_raw = pd.read_csv(filepath_or_buffer=\"../data/GlobalLandTemperaturesByCountry.csv\")\n",
    "df_temperature_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second dataset shows the numbers of asylum applications and asylum decisions taken by governments. There are two csv files: \"Asylum applications\" and \"Asylum decisions\". I used application data. (Link : https://www.kaggle.com/datasets/patrasaurabh/global-asylum-data-2000-present?select=asylum-applications.csv)\n",
    "\n",
    "This dataset has 6 columns, \"Year\", \"Country of origin\", \"Country of origin(ISO)\", \"Country of asylum\", \"Country of Asylum(ISO)\" and \"# applied\" and covers the years between 2000 and 2020.\n",
    "License of the dataset is CC0: Public Domain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Country of origin</th>\n",
       "      <th>Country of origin (ISO)</th>\n",
       "      <th>Country of asylum</th>\n",
       "      <th>Country of asylum (ISO)</th>\n",
       "      <th>applied</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2006</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>Australia</td>\n",
       "      <td>AUS</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2006</td>\n",
       "      <td>Albania</td>\n",
       "      <td>ALB</td>\n",
       "      <td>Australia</td>\n",
       "      <td>AUS</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2006</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>DZA</td>\n",
       "      <td>Australia</td>\n",
       "      <td>AUS</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2006</td>\n",
       "      <td>Egypt</td>\n",
       "      <td>EGY</td>\n",
       "      <td>Australia</td>\n",
       "      <td>AUS</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2006</td>\n",
       "      <td>Bahrain</td>\n",
       "      <td>BHR</td>\n",
       "      <td>Australia</td>\n",
       "      <td>AUS</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year Country of origin Country of origin (ISO) Country of asylum  \\\n",
       "0  2006       Afghanistan                     AFG         Australia   \n",
       "1  2006           Albania                     ALB         Australia   \n",
       "2  2006           Algeria                     DZA         Australia   \n",
       "3  2006             Egypt                     EGY         Australia   \n",
       "4  2006           Bahrain                     BHR         Australia   \n",
       "\n",
       "  Country of asylum (ISO)  applied  \n",
       "0                     AUS       14  \n",
       "1                     AUS       21  \n",
       "2                     AUS        5  \n",
       "3                     AUS       38  \n",
       "4                     AUS       11  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_asylum_raw = pd.read_csv(filepath_or_buffer=\"../data/asylum-applications.csv\")\n",
    "df_asylum_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Steps of Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline is designed to automate the process of reaching datasets from Kaggle, performing necessary transformations and uploading cleaned data to a SQLite database. Four steps of the pipeline:\n",
    "\n",
    "1-Download Datasets:\n",
    "Using the Kaggle API, datasets specified in the infos dictionary are downloaded and unzipped.\n",
    "\n",
    "2-Check File Integrity: \n",
    "The pipeline verifies if the datasets have been downloaded correctly.\n",
    "\n",
    "3-Transform Data: \n",
    "The downloaded datasets undergo specific transformations such as dropping unnecessary columns, extracting and filtering date information, and limiting the data to a specific range of years.\n",
    "\n",
    "4-Load into SQL:\n",
    "The cleaned and transformed data is loaded into a SQLite database for further analysis and use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Technologies Used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kaggle API:**\n",
    "For downloading datasets directly from Kaggle.\n",
    "\n",
    "**Pandas:** \n",
    "For data manipulation and transformation.\n",
    "\n",
    "**SQLAlchemy:** \n",
    "For interacting with the SQLite database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dropping Unnecessary Columns:** \n",
    "\n",
    "Columns that are not required for the analysis are dropped to save space and reduce complexity. This is done using the infos dictionary which specifies the columns to be dropped for each dataset. In temperature dataset 'AverageTemperatureUncertainty' column is dropped for the simplicity. In asylum dataset 'Country of origin (ISO)' and 'Country of asylum (ISO)' are dropped to prevent data redundancy.\n",
    "\n",
    "**Date Processing:**\n",
    "\n",
    "Conversion: The date column ('dt') is converted from string format to date format.\n",
    "\n",
    "Year Extraction: A new column 'Year' is created to store the year extracted from the date.\n",
    "\n",
    "Filtering: Data is filtered to include only the years between 2000 and 2012. This ensures the analysis is focused on a specific timeframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 - Problems Encountered and Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**File Download Issues:** Ensuring the files are correctly downloaded and unzipped. The pipeline includes a verification step to check the presence of the files in the directory. If a file is missing, it triggers a re-download.\n",
    "\n",
    "**Date Conversion:**\n",
    "Ensuring the date conversion is done correctly and efficiently. The 'pd.to_datetime' method is used to handle date parsing and extraction robustly.\n",
    "\n",
    "**Data Integrity:**\n",
    "Ensuring that data loaded into the database is clean and correctly formatted. The transformations applied ensure that only relevant data is included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 - Dealing with Error and Changing Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**File Existence Check:**\n",
    "Before proceeding with transformations, the pipeline checks if the necessary files are present in the specified directory. If not, it attempts to download them again.\n",
    "\n",
    "**Transformation Steps:**\n",
    "Each transformation step is designed to handle errors gracefully. For example, date conversion includes format specifications to handle different date formats.\n",
    "\n",
    "**Dynamic Handling:**\n",
    "The use of a dictionary (infos) to store dataset-specific information makes the pipeline adaptable to new datasets with minimal changes. By updating the dictionary with new dataset information, the pipeline can be easily extended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Result and Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 - Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output data of the pipeline consists of two cleaned and transformed datasets stored in two tables of a SQLite database. These datasets are:\n",
    "\n",
    "**Asylum Applications Data:** Contains information about asylum applications from various countries over a specified timeframe. Name of the table is 'asylum_applications'.\n",
    "\n",
    "**Global Temperature Data:** Contains historical temperature data by country. Name of the table is 'global_temperature'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Data Structure and Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Asylum Applications Data:**\n",
    "\n",
    "**Table Name:** asylum_applications\n",
    "\n",
    "**Columns:**\n",
    "\n",
    "*Country of origin:* The country from which asylum seekers originate.\n",
    "\n",
    "*Country of asylum:* The country in which asylum is sought.\n",
    "\n",
    "*Year:* The year of the application.\n",
    "\n",
    "Additional relevant columns from the original dataset.\n",
    "\n",
    "Quality: The data is filtered to include only relevant years (2000-2012), and unnecessary ISO code columns are removed. The quality of the data is maintained by focusing on key metrics and ensuring consistency in the date formats.\n",
    "\n",
    "**Global Temperature Data:**\n",
    "\n",
    "**Table Name:** global_temperature\n",
    "\n",
    "**Columns:**\n",
    "\n",
    "*Country:* The country for which temperature data is recorded.\n",
    "\n",
    "*AverageTemperature:* The average land temperature.\n",
    "\n",
    "*dt*: The original date column.\n",
    "\n",
    "*Year:* The extracted year from the date.\n",
    "\n",
    "Additional relevant columns from the original dataset.\n",
    "\n",
    "Quality: The data quality is improved by removing uncertainty columns and focusing on precise temperature measurements. The date column is converted and extracted into a year column to facilitate time-series analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - Data Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chosen output format for the pipeline is a SQLite database. This format is chosen for several reasons:\n",
    "\n",
    "**Structured Storage:** SQLite databases provide a structured way to store data, ensuring easy access and efficient querying.\n",
    "\n",
    "**Portability:** SQLite databases are self-contained, making them easy to share and deploy.\n",
    "\n",
    "**Scalability:** While SQLite is not intended for very large-scale applications, it is more than sufficient for moderate-sized datasets like those used in this pipeline.\n",
    "\n",
    "**Usage:** SQLite is widely used in the real world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 - Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Completeness:**\n",
    "\n",
    "The datasets are limited to the years 2000-2012. While this range is suitable for many analyses, it may exclude recent trends or historical contexts.\n",
    "Missing data in the original datasets could lead to incomplete records in the database. This needs to be addressed either by data imputation techniques or by acknowledging these gaps in the analysis.\n",
    "\n",
    "**Data Accuracy:**\n",
    "\n",
    "The transformations applied assume the initial datasets from Kaggle are accurate and well-structured. Any issues in the source data could propagate through the pipeline.\n",
    "Conversion errors, especially in date formats, could lead to inaccuracies in the transformed data. Thorough testing and validation are required to mitigate this.\n",
    "\n",
    "**Scalability and Performance:**\n",
    "\n",
    "While SQLite is adequate for the current data volume, scaling to larger datasets or concurrent access scenarios would require migrating to more robust database systems like PostgreSQL or MySQL.\n",
    "Performance bottlenecks could arise from extensive transformations, particularly date conversions. Optimization and efficient querying techniques should be considered.\n",
    "\n",
    "**Potential Bias:**\n",
    "\n",
    "The datasets may inherently carry biases, such as underreporting in asylum applications or temperature data limited to specific geographical regions. These biases should be acknowledged and accounted for in any analytical work.\n",
    "\n",
    "**Updating Data:**\n",
    "\n",
    "The pipeline currently downloads and processes static datasets. Incorporating mechanisms to handle updated datasets or real-time data streams would improve the pipeline’s utility.\n",
    "\n",
    "**Error Handling:**\n",
    "\n",
    "While basic error handling is implemented (e.g., re-downloading missing files), more robust error logging and handling mechanisms could enhance reliability. This includes catching and logging exceptions during data transformations and database operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLPProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
